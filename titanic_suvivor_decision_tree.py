# -*- coding: utf-8 -*-
"""titanic-suvivor-decision-tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xexQY4PXJaPm2590XIenZOsd50IoJjgr

#Titanic - Predicting Survival Machine Learning

On August 25th, 2022 I took an online introduction to data science course presented by [WeCloudData](https://weclouddata.com). In this course te instructor walked us through the creation of a simple decision tree model to predict passenger survial on the Titanic. This project is my adaptation and (slight!) refinement of that model.

##Imports
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

"""##Read Data"""

titanic = pd.read_csv('https://weclouddata.s3.amazonaws.com/data/titanic.csv')

"""##Review Data

### View Info

Row number, column names, non-null count, data type.
"""

titanic.info()

"""###View First Five Rows"""

titanic.head(5)

"""###View Dimensions & Indexing"""

titanic.shape

titanic.index

"""###View Column Names"""

titanic.columns

"""### View Statistical Summary

Statistical summary of numeric variables.
"""

titanic.describe()

"""###Count Missing Values"""

titanic.isnull().sum()

"""##Calculate & View Mean Age"""

mean_age = titanic['age'].mean()
mean_age

"""##Fill Missisng Age Values
As shown above, 263 age values are missing from the dataset. Null values can be filled with the mean age calculated above.
"""

titanic['age'] = titanic['age'].fillna(mean_age)
titanic

"""##Count Survivors by Sex"""

titanic.groupby('sex')['survived'].sum()

"""##Count of Passengers by Sex"""

titanic['sex'].value_counts()

"""##Save New Dataframe"""

titanic.to_csv('new_titanic.csv')

"""##Data Visualization

###Male vs. Female Passengers
"""

titanic['sex'].value_counts().plot(kind = 'bar');

"""###Age Distribution of Passengers"""

titanic['age'].plot(kind = 'hist');

"""###Died v. Survived 

0 = died, 1 = survived.
"""

titanic['survived'].value_counts().plot(kind = 'bar');

"""##Machine Learning

Simple decision tree model.
"""

from sklearn.datasets import make_blobs
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from mlxtend.plotting import category_scatter
from mlxtend.plotting import plot_decision_regions

X, y = make_blobs(centers=[[1, 1], [2, 2]], random_state=1)
df = pd.DataFrame(X, columns=['feature1', 'feature2']).assign(label=y)
df.head()

category_scatter(x='feature1', y='feature2', label_col='label', data=df);

"""max_depth value controls the level of fit. Overfitting occured at max_depth >= 3 and a max_depth = 1 underfits."""

d_tree = DecisionTreeClassifier(max_depth=2)
d_tree.fit(X, y)
plot_decision_regions(X, y, clf=d_tree);

import graphviz
from sklearn.tree import export_graphviz

dot_data = export_graphviz(d_tree, out_file=None, feature_names=['feature1', 'feature2'],  
                                filled=True, special_characters=True)  

graphviz.Source(dot_data)

"""###One Hot Encoding

Creates a column ('male') that expresses passenger sex numerically (female = 0, male = 1) so it can be fed into the ML model.
"""

new_sex = pd.get_dummies(titanic['sex'], drop_first=True)
titanic = pd.concat([titanic, new_sex], axis=1)
titanic

"""###Select Features & Create Features Dataset

During testing I also added 'embarked' as a fature but it made the model preform worse than expected.
"""

X = titanic[['pclass', 'age', 'sibsp', 'parch', 'fare', 'male']]
X

"""###Make Label"""

y = titanic['survived']

"""###Make Validation Dataset

2/3rds of dataset is used to train the model and 1/3rd is used for testing.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

"""###Build & Train *Model*"""

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

"""###Model Predictions"""

y_predict = model.predict(X_test)

"""###Model Evaluation

My model correctly predicted survival 76.2% of the time (approximately 996 passengers out a total of 1308). 
"""

accuracy_score(y_test, y_predict)

"""##Summary

This was my first attempt at creating an ML model and my first time coding in Python so I'm pretty happy with a 76.2% accuracy rate. 

I was suprised to see that the additon of 'embarked' actually made the model preform worse. In the future results may be improved by adding the feature 'cabin' to the model. Cabin numbers are noted in a letter plus number format (for example: B5). Extracting the letter, which refers to the deck level, could provide valuable insight. 

In the future it would also be worth exploring more sophisticated ML models.

"""